{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть А\n",
    "## Задача 1.\n",
    "\n",
    "В качестве функции потерь в задаче спользуется MSE (Mean Squared Error):\n",
    "$$\\mathit{MSE}=\\frac{1}{n} \\sum_{i=1}^n (y_{i}-\\hat{y_{i}})^2$$\n",
    "\n",
    "По условию для 80 из 100 случаев разница истинного и предсказанного значения составляет 0,5, а для остальных 20 случаев равняется -0,3. Тогда выразим предсказанные значения через истинные:\n",
    "\n",
    "$$\\hat{y_{i}}(80)=y_{i}-0.5$$\n",
    "$$\\hat{y_{i}}(20)=y_{i}+0.3$$\n",
    "\n",
    "Рассчитаем MSE для данного случая:\n",
    "$$\\mathit{MSE}=\\frac{1}{100} \\cdot ( \\sum_{i=1}^{80} (y_{i}-(y_{i}-0.5)^2 + \\sum_{i={80}}^{100} (y_{i}-(y_{i}+0.3)^2)$$\n",
    "\n",
    "$$\\mathit{MSE}=\\frac{1}{100} \\cdot ({80} \\cdot ({0.5})^2 + {20} \\cdot ({-0.3})^2) = 0.218$$\n",
    "\n",
    "Теперь добавим константу *с* к предсказанным значениям, и выразим разницу между истинными и предсказанными:\n",
    "\n",
    "$$y_{i}-((y_{i}-0.5)+c) = {0.5}-c$$\n",
    "$$y_{i}-((y_{i}+0.3)+c) = -{0.3}-c$$\n",
    "\n",
    "Мы хотим улучшить предсказания модели, а значит, минимизировать функцию потерь. MSE - это квадратичная функция, а значит ее минимум находится в точке, где производная функции равна нулю (более строго - где производная меняет знак). Итак, продифференцируем функцию и найдем значение константы *с* при минимальном значении функции:\n",
    "\n",
    "$$f'(\\frac{1}{100} \\cdot ({80} \\cdot ({0.5}-c)^2 + {20} \\cdot ({-0.3}-c)^2)) = 0$$\n",
    "$$\\frac{1}{100} \\cdot ({80}\\cdot 2 \\cdot {-1}\\cdot ({0.5}-c) + {20}\\cdot 2 \\cdot {-1} \\cdot ({-0.3}-c)=0$$\n",
    "$$\\frac{200 \\cdot c - {68}}{100}=0$$\n",
    "$$c=0.34$$\n",
    "\n",
    "Итак, при добавлении константы **с = 0.34** к предсказанным значениям, мы сможем добиться лучшего результата.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2.\n",
    "\n",
    "Отрицательные значения может возвращать градиентный бустинг, так как модель в ансамбле на каждой следующей итерации, начиная со второй, предсказывает остатки, полученные на предыдущей итерации.\n",
    "\n",
    "В начале необходимо получить первое предсказание. В случае регрессии — это может быть просто среднее значение целевой переменной (кол-во шагов в задаче) по всем данным. После этого алгоритм считает остатки модели, и все следующие модели (обычно деревья) предсказывают не таргетную переменную, а остатки, полученные на предыдущей итерации.\n",
    "\n",
    "Допустим, что на предыдущем шаге модель предсказала значения, больше, чем таргетная переменная, тогда остатки на этом шаге будут отрицательные. В таком случае следующая модель в качестве таргетной переменной будет использовать эти, отрицательные, остатки. Финальное предсказание основывается на \"общем решении\" всех деревьев, а потому можно получить отрицательные значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3.\n",
    "\n",
    "Гетероскедастичностью называют непостоянство дисперсии случайных ошибок модели. Это означает, что остатки распределены не случайно, в них отражается некий паттерн. Классический пример состоятельных и бедных людей показывает, что разнообразие потребляемых продуктов (а дисперсия как раз оценивает разнообразие в выборке) зависит от уровня дохода граждан: бедные люди могут позволить себя ограниченное разнообразие, а богатые - напротив.\n",
    "\n",
    "Параметры линейной регрессии оцениваются с использованием метода наименьших квадартов (МНК). Простыми словами, мы хотим подобрать такую линию регрессии, чтобы расстояние от нее до каждого наблюдений было минимальным.\n",
    "\n",
    "В общем случае ковариационная матрица МНК-оценок параметров линейной модели равна:\n",
    "\n",
    "$$V(\\hat {b}_{OLS})=(X^TX)^{-1}(X^TVX)(X^TX)^{-1}$$,\n",
    "\n",
    "где X — модельная матрица, а V — ковариационная матрица случайных ошибок.\n",
    "\n",
    "В случае присутствия гетероскедастичности ковариционная матрица случайных ошибок V является диагональной и все диагональные элементы  ${\\displaystyle \\sigma _{t}^{2}}$ неизвестны. В этом случае выражение для *V* можно представить так:\n",
    "\n",
    "$$V(\\hat {b}_{OLS})=(X^TX)^{-1}(\\sum_{t=1}^{n}\\sigma^2_{t}x_tx^T_t)(X^TX)^{-1}$$\n",
    "\n",
    "Было показано, что если вместо дисперсий ошибок (которые неизвестны) использовать квадраты остатков регрессии, то получится состоятельная оценка ковариационной матрицы. Такие остатки называют остатками Уайта.\n",
    "\n",
    "Однако, помимо дисперсии в остатках может быть обнаружена автокорреляция - скоррелированность остатков между собой. Автокорреляция может встретиться, например, при анализе временных рядов - где остатки могут иметь некую тенденцию или циклические колебания. При наличии гетероскедастичности и автокорреляции оценка Уайта оказывается несостоятельной - мы не можем обойтись лишь оценкой диагональных элементов матрицы, необходимо оуенить и внедиагональные элементы. Тогда используют стандартные ошибки в форме Ньюи-Уеста, и ковариацонная матрица ошибок приобратет вид:\n",
    "\n",
    "$$\\hat {V}(\\hat {b}_{OLS})=(X^TX)^{-1}(\\sum_{t=1}^{n}e^2_{t}x_tx^T_t+\\sum_{j=1}^{L}\\sum_{t=j+1}^{n}w_j e_t e_{t-j}(x_tx^T_{t-j}+x_{t-j}x^T_t))(X^TX)^{-1}$$\n",
    "\n",
    "Возможно, в задаче использование остатков в форме Уайта не приводит к улучшению модели как раз потому, что в остатках присутствует не только гетероскедастичность, но и автокорреляция?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 4.\n",
    "\n",
    "Стандартное отклонение показывает, насколько в среднем каждый элемент выборки отклоняется от среднего значения.\n",
    "Расчет стндартного отклонения выборки производится по следующей формуле:\n",
    "$$sd = \\sqrt{\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}{n-1}}$$\n",
    "\n",
    "Однако, вычисления по этой формуле могут быть неточными, и даже откровенно ложными - например, отрицательными. Если в нашей выборке содержатся огромные числа, а разница между ними очень маленькая, то при вычитании одного из другого могут возникнуть ошибки.\n",
    "\n",
    "Кроме того, вычислительно это будет очень долгий процесс - ведь каждый раз при добавлении нового элемента мы вынуждены повторять все расчеты заново.\n",
    "\n",
    "Воспользуемся алгоритмом Уэлфорда, основанном на динамическом программировании. Для начала докажем, что мы действительно можем получить стандартное отклонение при добавлении нового элеменьа, основываясь только на предыдущем значении - то есть выведем рекуррентную формулу.\n",
    "\n",
    "Среднее для n элементов можно легко выразить, зная среднее для n-1 элементов:\n",
    "\n",
    "$$x_n=\\frac{x_n-\\bar{x_{n-1}}}{n} + \\bar{x_{n-1}}$$\n",
    "\n",
    "\n",
    "Для того чтобы вывести стандартное отклонение, попробуем найти связь между суммой квадратов (числитель под корнем в формуле) для n-1 элементов и n элементов:\n",
    "\n",
    "$$ss_n - ss_{n-1} = \\sum_{i=1}^n\\left(x_i-\\bar{x_n}\\right)^2 - \\sum_{i=1}^{n-1}(x_i-\\bar{x_{n-1}})^2$$\n",
    "\n",
    "Заметим, что первое слагаемое можно выразить как:\n",
    "$$ss_n = \\sum_{i=1}^n(x_i-\\bar{x_n})^2 = (x_n-\\bar{x_n})^2 + \\sum_{i=1}^{n-1}(x_i-\\bar{x_n})^2$$\n",
    "\n",
    "Подставим полученное выражение:\n",
    "\n",
    "$$ss_n - ss_{n-1} = (x_n-\\bar{x_n})^2 + \\sum_{i=1}^{n-1}(x_i-\\bar{x_n})^2-\\sum_{i=1}^{n-1}(x_i-\\bar{x_{n-1}})^2$$\n",
    "\n",
    "Теперь займемся отдельно 2мя последними слагаемыми:\n",
    "\n",
    "$$\\sum_{i=1}^{n-1}(x_i-\\bar{x_n})^2 - \\sum_{i=1}^{n-1}(x_i-\\bar{x_{n-1}})^2=$$\n",
    "$$=\\sum_{i=1}^{n-1}(x_i-\\bar{x_n})^2 - (x_i-\\bar{x_{n-1}})^2=$$\n",
    "$$=\\sum_{i=1}^{n-1}(x_i-\\bar{x_n}+x_i-\\bar{x_{n-1}}) \\cdot (\\bar{x_{n-1}}-\\bar{x_n})=$$\n",
    "$$=(\\bar{x_{n-1}}-\\bar{x_n}) \\cdot \\sum_{i=1}^{n-1}(x_i-\\bar{x_n}+x_i-\\bar{x_{n-1}})$$\n",
    "\n",
    "Преобразуем отдельно последний множитель:\n",
    "\n",
    "$$\\sum_{i=1}^{n-1}(x_i-\\bar{x_n}+x_i-\\bar{x_{n-1}})=\\sum_{i=1}^{n-1}(x_i-\\bar{x_n})+\\sum_{i=1}^{n-1}(x_i-\\bar{x_{n-1}})$$\n",
    "\n",
    "Заметим, что второе слагаемое равно нулю (сумма отклонений всех значений от среднего = 0). Распишем первое слагаемое:\n",
    "\n",
    "$$\\sum_{i=1}^{n-1}(x_i-\\bar{x_n}) + (x_n-\\bar{x_n}) - (x_n-\\bar{x_n})=$$\n",
    "$$=(\\sum_{i=1}^{n-1}(x_i-\\bar{x_n})+ x_n-\\bar{x_n}) - (x_n-\\bar{x_n})=$$\n",
    "$$=\\sum_{i=1}^{n}(x_i-\\bar{x_n}) - (x_n-\\bar{x_n})= 0 -(x_n-\\bar{x_n})=-(x_n-\\bar{x_n})$$\n",
    "\n",
    "Вернем полученное значение в исходную формулу:\n",
    "\n",
    "$$ss_n - ss_{n-1}=(x_n-\\bar{x_n})^2-(\\bar{x_{n-1}}-\\bar{x_n}) \\cdot (x_n-\\bar{x_n})=(x_n-\\bar{x_n})\\cdot(x_n-\\bar{x_{n-1}})$$\n",
    "\n",
    "Итак, мы действительно можем выразить сумму квадратов при добавлении n-ого элемента через среднее значение n-1 и n элементов. Получаем 2 формулы: для среднего и стандартного отклонения.\n",
    "\n",
    "$$x_n=\\frac{x_n-\\bar{x_{n-1}}}{n} + \\bar{x_{n-1}}$$\n",
    "$$ss_n=(x_n-\\bar{x_n})\\cdot(x_n-\\bar{x_{n-1}})+ss_{n-1}$$\n",
    "\n",
    "Код находится в папке **task_A4**. В файле **refrigerator.py** находятся 2 класса: Stream и WelfordStd, объекты которых необходимы для вычислений. В файле **welford_sd.py** находится скрипт, который запускается из командной строки. Он принимает на вход значения по одному, после каждого выдает стандартное отклонение, а также объем памяти, затраченный им на вычисление (спойлер: всего ~8Mb)\n",
    "\n",
    "Пример запуска:\n",
    "```console \n",
    "python welford_sd.py\n",
    "```\n",
    "Кроме того я сравнила по времени алгоритм Уэлфорда со стандартным алгоритмом, имплементированным в библиотеке ```numpy```. Код для сравнения и построения графика находится в файле **time_compare_welford_numpy.py** Выяснилось, что разница колоссальная:\n",
    "\n",
    "![Welford](Welford_numpy_std.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 5.\n",
    "\n",
    "Для решения задачи создадим базу данных, похожую на пример, и подберем к ней запрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection (new db)\n",
    "connection = sqlite3.connect('tasks_date.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table in db\n",
    "query = '''CREATE TABLE IF NOT EXISTS tasks ('dt' TEXT, 'task_id' INTEGER PRIMARY KEY)'''\n",
    "connection.execute(query)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert a lot of data\n",
    "query = '''INSERT INTO tasks ('dt', 'task_id') VALUES (?, ?)'''\n",
    "\n",
    "learners = [('2020-05-31 11:00:02', 1),\n",
    "            ('2020-05-31 11:00:03', 2),\n",
    "            ('2020-05-31 11:00:04', 332),\n",
    "            ('2020-05-31 11:00:05', 169),\n",
    "            ('2020-05-31 11:00:06', 70),\n",
    "            ('2020-05-31 11:00:07', 100),]\n",
    "connection.executemany(query, learners)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, число задач 100, а число попыток - не больше 10. Тогда отберем все ```task_id``` значения которых больше, чем число задач, а затем возьмем остаток от деления каждого такого ```task_id``` на число задач. В итоге получим исходный ```task_id``` для всех задач, исполнение которых произошло не с первого раза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''SELECT task_id % 100 FROM tasks WHERE task_id > 100'''\n",
    "res = connection.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69,)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for row in res.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче первая строка содержала ```task_id``` = 0. Мне кажется, это не совсем корректно. Допустим, что имеется 100 задач. При этом 0-я задача не исполнилась с первого раза, а исполнилась со второго - и получила ```task_id``` = 100. При этом 100-я задача исполнилась с первого раза, и также получила ```task_id``` = 100. В итоге двум разным задачам присвоены одинаковые ```task_id```, что нехорошо!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть В"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправленный код находится в папке ```part_B```. Основной баг заключался в неправильном использовании ```time.sleep()``` - ```counter``` спал в то время, когда надо было собирать статситику, а ```task```, не дождидаясь, проводил дальнейшие вычисления. Это привело к рассинхронизации потоков и неправильному ответу на выходе. Кроме основного бага я почистила код стилистически (по PEP-8), а также заменила создание списка на list comprehension, так как этот способ точно сработает верно, если работать с двумерными списками."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
